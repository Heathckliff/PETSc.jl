%\documentclass[letter,12pt]{article}
\documentclass{article}
%\usepackage[margin=0.6in]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{color}
\usepackage{minted}
\usepackage{fullpage}
\usepackage{subfigure}
\usepackage{float}
\usepackage{verbatim}
\usepackage{courier}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{placeins}
\usepackage{units}
\newcommand{\ttt}{\texttt}
\newcommand{\die}{\partial}

\usepackage{cite}

%\renewcommand{\citeleft}{\textcolor{red}{[}}
%\renewcommand{\citeright}{\textcolor{red}{]}}

%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{hyperref}
\hypersetup{
 colorlinks=true,
 citecolor=blue,
 linkcolor=red,
 urlcolor=blue}


\title{Julia-Petsc Interface III}
\begin{comment}
\author{
  Jared Crean\thanks{Graduate Student, Rensselaer Polytechnic Institute}, \
  Katharine Hyatt\thanks{Graduate Student, University of California Santa Barbara}, \ and
  Steven G. Johnson\thanks{Associate Professor, Massachusetts Institute of Technology},
}
\end{comment}

\date{\today}

\begin{document}
\maketitle

\section{Introduction} \label{sec:intro}
This document describes the changes to the Julia interface to the Portable, Extensible Toolkit for Scientific Computation (PETSc) since the previous report.
New functionality in the form of a new vector type, build system improvements, and indexing changes are included, as well as an interface to the PETSc Time Stepping (TS)
component.


\section{Vectors}
\subsection{Constructors}
PETSc ghost vectors are now supported via the outer constructor

\begin{minted}{julia}
function VecGhost{T<:Scalar, I <: Integer}(::Type{T}, mlocal::Integer, 
                  ghost_idx::Array{I,1}; comm=MPI.COMM_WORLD, m=C.PETSC_DECIDE, bs=1)
\end{minted}
\noindent which allows the user to specify the number of entries in the vector 
that are locally owned and the global indices of entries owned by other 
processes whose data will be needed by the current process.  This is useful
for eg. finite difference stencils near a parallel boundary, to ensure all the 
data needed for the stencil is available locally. After the values have been
updated, the functions

\begin{minted}{julia}
function ghost_begin!{T<:Scalar}(v::Vec{T, C.VECMPI}; imode=C.INSERT_VALUES,
                                 smode=C.SCATTER_FORWARD)

function ghost_end!{T<:Scalar}(v::Vec{T, C.VECMPI}; imode=C.INSERT_VALUES,
function scatter!{T<:Scalar}(v::Vec{T, C.VECMPI}; imode=C.INSERT_VALUES, 
                             smode=C.SCATTER_FORWARD)
\end{minted}

\noindent are used to update remote copies of the values after changes are made
to the local values.  The first two functions begin and end the ghost update, 
while the third is a convenience method for calling both in a single function 
call.  It should be noted that calling \texttt{ghost\_begin!}, doing some 
local computation, and then calling \texttt{ghost\_end!}, can be more efficient
than calling \ttt{scatter!} because non-blocking communication may be used to
send the data while the computation is taking place.  
Also, the \texttt{smode} parameter controls the direction of
the update, ie. copying owner values to the ghosts or ghost values to the owner.
\texttt{imode} controls whether the new values are added to the existing 
ones or overwrite them.

\subsection{Map}
The \texttt{Base} functions for mapping a function over a vector are extended
to PETSc vectors:

\begin{minted}{julia}
function map!(f, c)
function map!{T}(f, dest::Vec{T}, src::Vec)
function map!{T, T2}(f, dest::Vec{T}, src1::Vec{T}, src2::Vec{T2},  
                     src_rest::Vec{T2}...)
\end{minted}

\noindent and the method
\begin{minted}{julia}
function map(f, c)
\end{minted}

\noindent is inherited from \texttt{Base}.

These functions allow applying a function \texttt{f} element-wise to a vector.
The first method does the operation in-place on the vector, the second stores
the result in an already-created vector \texttt{dest}, the third method 
takes a function of \texttt{n} arguments, where \texttt{n} is the number of
\texttt{src} vectors, and stores the result in \texttt{dest}, and the final 
method allocates a new vector to store the result and returns it.
In parallel, each process applies the function to the local part of the vector.
If the vector contains ghost values, the function is applied to them as well.
The process is extremely efficient via the use of \texttt{LocalArray}s, which
allow accessing the memory underlying the local part of a PETSc vector directly,
as though it were a native Julia vector.

\section{Indexing and Assembly}
In previous versions of PETSc.jl, the functions \texttt{AssemblyBegin} and 
\texttt{AssemblyEnd} were called after each call to \texttt{MatSetValues} and
\texttt{VecSetValues} via indexing notation.  Because these are collective 
operations, all processes had to making the same number of indexing calls, 
otherwise the code would deadlock, and would generally cause very poor parallel
performance due to network latency.

To address this, a new field was added 
to both the \texttt{Mat} and \texttt{Vec} types, called 
\texttt{verify\_assembled}, which controls how the \texttt{isassembled} checks
the assembly status of the object.  Instead of having \texttt{setindex!}
assemble the object, functions that require the object to be assembled, such
as the linear solver, must 
check the assembly state and assemble if required.  
By default, \texttt{verify\_assembled} is true, and \texttt{isassembled} does
an \texttt{MPI\_Allreduce} to verify that all processes are assembled.  If
false, each process only checks whether the locally owned part of the object
is assembled, a check that is practically free.  When the user sets the 
value to false, they are effectively agreeing to manage assembly of the 
object themselves.  This can be beneficial to performance, especially in cases
where object assembly can be overlapped with computation.  Using the default 
value of true, the PETSc.jl package will do all necessary checks to ensure
the proper assembly state, while allowing users to set the value to false to 
avoid the extra \texttt{MPI\_Allreduce} calls and attain better performance.

\section{Build System}
The build system has been improved to increase flexibility and user input, by
recognizing certain environmental variables.  This has several facets.  
First, the build system can be directed to use an existing build of PETSc 
rather than build a new one by setting the \texttt{PETSC\_DIR} and 
\texttt{PETSC\_ARCH} for that build in the variables 
\texttt{JULIA\_PETSC\_name\_DIR} and \texttt{JULIA\_PETSC\_name\_ARCH}, where \texttt{name} identify the datatype of the build (\texttt{RealDouble}, 
\texttt{RealSinge}, or \texttt{ComplexDouble}.  This can be useful on compute
clusters or supercomputers where PETSc is already installed.  

Second, the build system can be directed to not build a version of PETSc using
the variable \texttt{JULIA\_PETSC\_name\_NOBUILD}.  This saves build time, 
disk space, and test time if PETSc matrices and vectors with only one datatype,
 such as Float64, is needed.

Third, arbitrary user flags can be passed to the PETSc configure script via
the \texttt{JULIA\_PETSC\_FLAGS}.  This enables, for example, downloading 
any of the numerous packages PETSc can interface with, as well as performing 
an optimized build of PETSc itself.  By default, a debug build is performed, 
which does additional error checking at the expense of some performance.  
As a convenience, the \texttt{JULIA\_PETSC\_OPT}, variable can be set to tell
the build system to use a set of default optimization flags.

\section{TS}
An interface to the PETSc component for time-stepping Ordinary Differential
Equations (ODEs) and Differential Algebraic Equations (DAEs) has been created.
The TS time is defined as

\begin{minted}{julia}
type TS{T}
  p::C.TS{T}
  data::Array{Any, 1}  # hold the various ctx tuples
end
\end{minted}

Several outer constructors are available

\begin{minted}{julia}
function TS{T<:Scalar}(::Type{T}; comm=MPI.COMM_WORLD)
function TS{T<:Scalar}(::Type{T}, tsptype::C.TSProblemType; comm=MPI.COMM_WORLD)
function TS{T<:Scalar}(::Type{T}, tsptype::C.TSProblemType, tstype::C.TSType; 
                       comm=MPI.COMM_WORLD)
\end{minted}

\noindent which offer increasing levels of specificity.  The first constructor
takes the problem type and the time stepping method from the PETSc options 
database, where the problem type tells PETSc whether the system is linear or
non-linear. The second constructor takes the problem type as an argument and
the method from the options database, and the third one takes both as arguments.

The function
\begin{minted}{julia}
function set_ic{T<:Scalar}(ts::TS{T}, u::Vec{T}) 
\end{minted}

\noindent uses the current value of \texttt{u} as the initial condition for 
the time stepping.

The function
\begin{minted}{julia}
function set_times{T<:Scalar}(ts::TS{T}, t0, dt0,  nsteps::Integer, tmax)
\end{minted}

\noindent sets the initial time value, the time step sizes, the maximum number
of time steps, and the maximum time value for the time stepping run.

To solve the problem, two functions are available
\begin{minted}{julia}
function solve!{T}(ts::TS{T})
function solve!{T}(ts::TS{T}, vec::Vec{T})
\end{minted}

The first method is uses in conjunction with \texttt{set\_ic}, while the 
second takes the initial condition in the vector \texttt{vec}.

The system being time-stepped is described to PETSc in two parts.  The first is 
the right hand side, which is time stepped explicitly, and the second is the 
left hand side, which is time stepped implicitly.  In general, PETSc requires 
functions to evaluate the right and left hand side functions and their 
Jacobians  in order to time step.  These functions are provided to PETSc via

\begin{minted}{julia}
function set_rhs_function{T}(ts::TS{T}, r::Vec{T}, f::Function, ctx=())
function set_rhs_jac{T}(ts::TS{T}, A::Mat{T}, B::Mat{T}, f::Function, ctx=())
function set_lhs_function{T}(ts::TS{T}, res::Vec{T}, f::Function, ctx::Tuple=())
function set_lhs_jac{T}(ts::TS{T}, A::Mat{T}, B::Mat{T}, f::Function, ctx::Tuple=())
\end{minted}
For \texttt{set\_rhs\_function}, the function \texttt{f} must have the 
signature

\begin{minted}{julia}
function f(ts::TS, t::Real, u::Vec, F::Vec, ctx)
\end{minted}

where \texttt{u} is the vector containing the current solution and \texttt{F}
is the vector to be populated with the right hand side function.  \texttt{ctx}
is the tuple passed into \texttt{set\_rhs\_function}.  The tuple provides a 
mechanism to pass any data needed by the function though PETSc to the function.
\texttt{t} is current time value.

For \texttt{set\_rhs\_jac}, the function must have the signature

\begin{minted}{julia}
function f(ts::TS, t::Real, u::Vec, A::mat, B::Mat, ctx)
\end{minted}

where \texttt{u} is the same as before, \texttt{A} and \texttt{B} are the 
Jacobian matrices for the linear solve and pre-conditioning, respectively.  
\texttt{ctx} is the tuple passed into \texttt{set\_rhs\_jac}.

The function for \texttt{set\_lhs\_function} must have the signature

\begin{minted}{julia}
function f(ts::TS, t::Real, u::Vec, ut::Vec, F::Vec, ctx)
\end{minted}
where all the arguments are the same as the function for 
\texttt{set\_right\_function} with the addition of \texttt{ut}, which contains
the current time derivative of the solution.

For \texttt{set\_lhs\_jac}, the function must have the signature

\begin{minted}{julia}
function f(ts::TS, u::Vec, ut::Vec, a::Real, A::Mat, B::Mat, ctx)
\end{minted}
where \texttt{a} is the shift applied to the diagonal of the matrix, as 
described in the PETSc documentation.  All other arguments are as described
previously.

For some time stepping methods such as explicit Euler, the Jacobian is not 
required.  In this case the function \texttt{PETSc.ComputeRHSJacobianConstant}
can be passed into \texttt{set\_rhs\_jac}.

It is important to note that the functions do not need to be c-callable, 
because of the use of wrappers within the PETSc.jl package.
\end{document}






